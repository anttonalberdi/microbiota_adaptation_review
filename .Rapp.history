sum(!complete.cases(bold_asv))
sum(complete.cases(bold_asv)
)
complete.cases(bold_asv)
sum(bold_asv =="")
sum(ncbi_asv =="")
asv="X"
bold_asv <- bold_table[bold_table$OTU == asv,]
bold_asv
nrow(bold_asv)
nrow(ncbi_asv)
#Load tables#
bold_table <- read.table("/Users/anttonalberdi/Downloads/ZBJ.tax.final-1.tsv",header=TRUE, sep="\t")#
ncbi_table <- read.table("/Users/anttonalberdi/Downloads/ZBJ_nt.tax.final-1.tsv",header=TRUE, sep="\t")#
#
#Remove duplicates#
bold_table <- bold_table[!duplicated(bold_table),]#
ncbi_table <- ncbi_table[!duplicated(ncbi_table),]
#Identify ASVs and sort them#
asvlist <- mixedsort(unique(c(as.character(bold_table[,1]),as.character(ncbi_table[,1]))))#
#
#Iterate across ASVs#
for (asv in asvlist){#
  #Subset table#
  bold_asv <- bold_table[bold_table$OTU == asv,]#
  ncbi_asv <- ncbi_table[ncbi_table$OTU == asv,]#
#
  #If BOLD is empty use NCBI#
  if(nrow(bold_asv) == 0){#
  output_asv <- ncbi_asv#
  #If NCBI is empty use BOLD#
  }else if(nrow(ncbi_asv) == 0){#
  output_asv <- bold_asv#
  }else{#
  #If both taxonomies are present:#
    #If taxonomies are identical just print one#
    if(all(bold_asv[c(3:8)] == ncbi_asv[c(3:8)])){#
    output_asv <- bold_asv#
    }else{#
      #If taxonomies are different select the one with highest identity#
      if(bold_asv[,2] > ncbi_asv[,2]){#
      output_asv <- bold_asv#
      }else if(bold_asv[,2] < ncbi_asv[,2]){#
      output_asv <- ncbi_asv#
      }else{#
        #If the identities are identical, select the most complete taxonomy#
        if(sum(bold_asv=="") < sum(ncbi_asv=="")){#
        output_asv <- bold_asv#
        }else if(sum(bold_asv=="") > sum(ncbi_asv=="")){#
        output_asv <- ncbi_asv#
      }else{#
        #If the criteria cannot choose any preferred taxonomy, use BOLD for being better curated#
        output_asv <- bold_asv#
      }#
    }#
  }#
  }#
}
#Load tables#
bold_table <- read.table("/Users/anttonalberdi/Downloads/ZBJ.tax.final-1.tsv",header=TRUE, sep="\t")#
ncbi_table <- read.table("/Users/anttonalberdi/Downloads/ZBJ_nt.tax.final-1.tsv",header=TRUE, sep="\t")#
#
#Remove duplicates#
bold_table <- bold_table[!duplicated(bold_table),]#
ncbi_table <- ncbi_table[!duplicated(ncbi_table),]#
#
#Identify ASVs and sort them#
asvlist <- mixedsort(unique(c(as.character(bold_table[,1]),as.character(ncbi_table[,1]))))#
#
merged_taxonomy <- c()#
#Iterate across ASVs#
for (asv in asvlist){#
  #Subset table#
  bold_asv <- bold_table[bold_table$OTU == asv,]#
  ncbi_asv <- ncbi_table[ncbi_table$OTU == asv,]#
#
  #If BOLD is empty use NCBI#
  if(nrow(bold_asv) == 0){#
  output_asv <- ncbi_asv#
  #If NCBI is empty use BOLD#
  }else if(nrow(ncbi_asv) == 0){#
  output_asv <- bold_asv#
  }else{#
  #If both taxonomies are present:#
    #If taxonomies are identical just print one#
    if(all(bold_asv[c(3:8)] == ncbi_asv[c(3:8)])){#
    output_asv <- bold_asv#
    }else{#
      #If taxonomies are different select the one with highest identity#
      if(bold_asv[,2] > ncbi_asv[,2]){#
      output_asv <- bold_asv#
      }else if(bold_asv[,2] < ncbi_asv[,2]){#
      output_asv <- ncbi_asv#
      }else{#
        #If the identities are identical, select the most complete taxonomy#
        if(sum(bold_asv=="") < sum(ncbi_asv=="")){#
        output_asv <- bold_asv#
        }else if(sum(bold_asv=="") > sum(ncbi_asv=="")){#
        output_asv <- ncbi_asv#
      }else{#
        #If the criteria cannot choose any preferred taxonomy, use BOLD for being better curated#
        output_asv <- bold_asv#
      }#
    }#
  }#
  }#
merged_taxonomy <- rbind(merged_taxonomy,output_asv)#
}
head(merged_taxonomy)
head(merged_taxonomy,50)
write.table(merged_taxonomy,"/Users/anttonalberdi/Downloads/ZBJ.merged.tax.tsv")
write.table(merged_taxonomy,"/Users/anttonalberdi/Downloads/ZBJ.merged.tax.tsv",quotes=FALSE)
write.table(merged_taxonomy,"/Users/anttonalberdi/Downloads/ZBJ.merged.tax.tsv",quote=FALSE)
write.table(merged_taxonomy,"/Users/anttonalberdi/Downloads/ZBJ.merged.tax.tsv",quote=FALSE,sep="\t")
write.table(merged_taxonomy,"/Users/anttonalberdi/Downloads/ZBJ.merged.tax.tsv",quote=FALSE,sep="\t",rownames=TRUE,colnames=TRUE)
write.table(merged_taxonomy,"/Users/anttonalberdi/Downloads/ZBJ.merged.tax.tsv",quote=FALSE,sep="\t",row.names=TRUE,col.names=TRUE)
head(merged_taxonomy)
write.table(merged_taxonomy,"/Users/anttonalberdi/Downloads/ZBJ.merged.tax.tsv",quote=FALSE,sep="\t",row.names=FALSE,col.names=TRUE)
#Load tables#
bold_table <- read.table("/Users/anttonalberdi/Downloads/ANML.tax.final-1.tsv",header=TRUE, sep="\t")#
ncbi_table <- read.table("/Users/anttonalberdi/Downloads/ANML_nt.tax.final-1.tsv",header=TRUE, sep="\t")#
#
#Remove duplicates#
bold_table <- bold_table[!duplicated(bold_table),]#
ncbi_table <- ncbi_table[!duplicated(ncbi_table),]#
#
#Identify ASVs and sort them#
asvlist <- mixedsort(unique(c(as.character(bold_table[,1]),as.character(ncbi_table[,1]))))#
#
merged_taxonomy <- c()#
#Iterate across ASVs#
for (asv in asvlist){#
  #Subset table#
  bold_asv <- bold_table[bold_table$OTU == asv,]#
  ncbi_asv <- ncbi_table[ncbi_table$OTU == asv,]#
#
  #If BOLD is empty use NCBI#
  if(nrow(bold_asv) == 0){#
  output_asv <- ncbi_asv#
  #If NCBI is empty use BOLD#
  }else if(nrow(ncbi_asv) == 0){#
  output_asv <- bold_asv#
  }else{#
  #If both taxonomies are present:#
    #If taxonomies are identical just print one#
    if(all(bold_asv[c(3:8)] == ncbi_asv[c(3:8)])){#
    output_asv <- bold_asv#
    }else{#
      #If taxonomies are different select the one with highest identity#
      if(bold_asv[,2] > ncbi_asv[,2]){#
      output_asv <- bold_asv#
      }else if(bold_asv[,2] < ncbi_asv[,2]){#
      output_asv <- ncbi_asv#
      }else{#
        #If the identities are identical, select the most complete taxonomy#
        if(sum(bold_asv=="") < sum(ncbi_asv=="")){#
        output_asv <- bold_asv#
        }else if(sum(bold_asv=="") > sum(ncbi_asv=="")){#
        output_asv <- ncbi_asv#
      }else{#
        #If the criteria cannot choose any preferred taxonomy, use BOLD for being better curated#
        output_asv <- bold_asv#
      }#
    }#
  }#
  }#
merged_taxonomy <- rbind(merged_taxonomy,output_asv)#
}
write.table("/Users/anttonalberdi/Downloads/ANML.merged.tax.tsv",opt$o,quote=FALSE,sep="\t",row.names=FALSE,col.names=TRUE)
write.table("/Users/anttonalberdi/Downloads/ANML.merged.tax.tsv",quote=FALSE,sep="\t",row.names=FALSE,col.names=TRUE)
write.table(merged_taxonomy,"/Users/anttonalberdi/Downloads/ANML.merged.tax.tsv",quote=FALSE,sep="\t",row.names=FALSE,col.names=TRUE)
~dirFCM1_18.csvdir=
dir="/cliptest/2-Filtered/A/"
filtFs_list <- gsub("_1.fastq","",list.files(path = dir, pattern = "_1.fastq", full.names=TRUE))#
filtRs_list <- gsub("_2.fastq","",list.files(path = dir, pattern = "_2.fastq", full.names=TRUE))#
filtFs_rev_list <- gsub("_1.rev.fastq","",list.files(path = dir, pattern = "_1.rev.fastq", full.names=TRUE))#
filtRs_rev_list <- gsub("_2.rev.fastq","",list.files(path = dir, pattern = "_2.rev.fastq", full.names=TRUE))
filtRs_rev_list
filtFs_list
dir
filtFs_list <- gsub("_1.fastq","",list.files(path = dir, pattern = "_1.fastq", full.names=TRUE))
filtFs_list
list.files(path = dir, pattern = "_1.fastq", full.names=TRUE)
dir
dir="/Users/anttonalberdi/cliptest/2-Filtered/A/"
filtFs_list <- gsub("_1.fastq","",list.files(path = dir, pattern = "_1.fastq", full.names=TRUE))
filtFs_list
filtFs_rev_list <- gsub("_1.rev.fastq","",list.files(path = dir, pattern = "_1.rev.fastq", full.names=TRUE))
filtFs_rev_list
if (setequal(filtFs_list, filtRs_list) == TRUE){#
  filtFs <- paste(filtFs_list,"_1.fastq",sep="")#
  filtRs <- paste(filtFs_list,"_2.fastq",sep="")#
}else if(setequal(filtFs_rev_list, filtRs_rev_list) == TRUE){#
  filtFs <- c(filtFs,paste(filtFs_rev_list,"_1.rev.fastq",sep=""))#
  filtRs <- c(filtRs,paste(filtRs_rev_list,"_1.rev.fastq",sep=""))#
}else{#
  write("ERROR! The forward and reverse reads do not match",file=statsfile,append=TRUE)#
  print("ERROR! The forward and reverse reads do not match")#
}
filtFs <- paste(filtFs_list,"_1.fastq",sep="")#
  filtRs <- paste(filtFs_list,"_2.fastq",sep="")
filtFs <- c(filtFs,paste(filtFs_rev_list,"_1.rev.fastq",sep=""))#
  filtRs <- c(filtRs,paste(filtRs_rev_list,"_1.rev.fastq",sep=""))
filtFs
pattern="fastq"
filtFs_list <- gsub(paste("_1.",pattern,sep=""),"",list.files(path = dir, pattern = paste("_1.",pattern,sep=""), full.names=TRUE))
filtFs_list
filtFs <- paste(filtFs_list,paste("_1.",pattern,sep=""),sep="")
filtFs
filtRs <- paste(filtFs_list,paste("_2.",pattern,sep=""),sep="")
filtRs
setequal(filtFs_list, filtRs_list)
filtFs_list <- gsub(paste("_1.",pattern,sep=""),"",list.files(path = dir, pattern = paste("_1.",pattern,sep=""), full.names=TRUE))#
filtRs_list <- gsub(paste("_2.",pattern,sep=""),"",list.files(path = dir, pattern = paste("_2.",pattern,sep=""), full.names=TRUE))
filtFs_list
filtRs_list
setequal(filtFs_list, filtRs_list)
dir
filtFs_list <- gsub(paste("_1.",pattern,sep=""),"",list.files(path = dir, pattern = paste("_1.",pattern,sep=""), full.names=TRUE))#
filtRs_list <- gsub(paste("_2.",pattern,sep=""),"",list.files(path = dir, pattern = paste("_2.",pattern,sep=""), full.names=TRUE))
filtFs_list
filtRs_list
if (setequal(filtFs_list, filtRs_list) == TRUE){#
  filtFs <- paste(filtFs_list,paste("_1.",pattern,sep=""),sep="")#
  filtRs <- paste(filtFs_list,paste("_2.",pattern,sep=""),sep="")#
}else{#
  print("ERROR! The forward and reverse reads do not match")#
}
filtFs
filtRs
paste("_1.",pattern,sep="")
dir="/Users/anttonalberdi/cliptest/3-DADA2"
filelist <- list.files(path = dir, pattern = ".rds", full.names=TRUE)#
SequenceTableList <- lapply(filelist,readRDS)
library(dada2)
trimtablist <- c("A.rev.csv","B.rev.csv","C.csv")
trimtablist[!grepl("rev.csv",trimtablist)]
table <- read.csv("~/Downloads/exercise.csv")
table
table <- read.csv("~/Downloads/exercise.csv",row.names=1,header=TRUE)
table
library(g)
library(hilldiv)
hill_div(table,qvalue=0)
hill_div(table,qvalue=1)
hill_div(table,qvalue=2)
table <- read.csv("~/Downloads/exercise.csv",row.names=1,header=TRUE)
table
library(hilldiv)
hill_div(table,qvalue=0)
hill_div(table,qvalue=1)
hill_div(table,qvalue=2)
hill_div(table,qvalue=0.5)
hill_div(table,qvalue=0)
hill_div(table,qvalue=0.1)
hill_div(table,qvalue=0.2)
hill_div(table,qvalue=0.32)
hill_div(table,qvalue=0.4)
hill_div(table,qvalue=0.5)
hill_div(table,qvalue=1)
hill_div(table,qvalue=2)
base <- c(496,134,548,414,962,859,1784)
hist(base)
dlnorm(4, meanlog = 0, sdlog = 1)
t.test(c(1.87,2.42,2.92),c(3.29,2.8,3.33,2.96))
hist(dnorm(100, 2028.8, 322.37))
dnorm(100, 2028.8, 322.37)
dnorm(c(1:1000), 2028.8, 322.37)
hist(dnorm(c(1:1000), 2028.8, 322.37))
hist(rnorm(1000, 2028.8, 322.37))
hist(rnorm(1000, 2661.4, 265.39))
hist(rnorm(1000, 2562.3, 368.56))
hist(rnorm(1000, 2076.7, 343.17))
pdf("hist.pdf",weight=8,height=6)
pdf("hist.pdf",width=8,height=6)
hist(rnorm(1000, 2076.7, 343.17))
dev.off()
pdf("hist.pdf",width=8,height=6)
hist(rnorm(1000, 2212.6, 328.22))
dev.off()
library(hier.part )
library(hier.part)
install.packages("hier.part")
library(hier.part)
data(urbanwq)#
    env <- urbanwq[,2:8]
env
example <- read.csv(""~/Downloads/example.csv")
example <- read.csv("~/Downloads/example.csv")
example
example <- read.csv("~/Downloads/example.csv")
example
aov(Chicken.body.weight ~ Pen * Replicate * Trial, data = example)
summary(aov(Chicken.body.weight ~ Pen * Replicate * Trial, data = example))
summary(aov(Chicken.body.weight ~ Pen + Replicate + Trial, data = example))
library(lme4)
fit = lmer(Chicken.body.weight ~ Pen + (Pen | Trial),  example)
fit
VarCorrCI(fit)
library(nlme)
subset <- read.csv("~/Downloads/samples.tsv")
head(subset)
subset <- read.csv("~/Downloads/samples.tsv",row.names=1)
head(subset)
subset <- read.csv("~/Downloads/samples.tsv")
head(subset)
subset <- read.tabe("~/Downloads/samples.tsv",row.names=1)
subset <- read.table("~/Downloads/samples.tsv",row.names=1)
head(subset)
asvtable <- read.table("~/Downloads/ASVs_counts_default_223_223.tsv")
head(asvtable)
colnames(asvtable)
samples <- colnames(asvtable)
head(samples)
samples <- grep(samples,"_rev$")
samples <- colnames(asvtable)#
samples <- grep("_rev$",samples)
samples
samples <- grep("_rev$",samples,values=T)
samples <- colnames(asvtable)#
samples <- grep("_rev$",samples,values=T)
samples <- colnames(asvtable)#
samples <- grep("_rev$",samples,value=T)
samples
samples <- grep("_rev$",samples,value=T)#
samples <- gsub("_rev$","",samples)
samples
samples <- colnames(asvtable)#
samples1 <- grep("_rev$",samples,value=T)#
samples2 <- gsub("_rev$","",samples1)
samples1
samples <- colnames(asvtable)#
samples2 <- grep("_rev$",samples,value=T)#
samples1 <- gsub("_rev$","",samples2)
samples1
samples2
samples <- sort(is colnames(asvtable))#
samples2 <- grep("_rev$",samples,value=T)#
samples1 <- gsub("_rev$","",samples2)
samples <- sort(colnames(asvtable))#
samples2 <- grep("_rev$",samples,value=T)#
samples1 <- gsub("_rev$","",samples2)
samples1
samples2
length(samples1)
i=1
sum(asvtable[,samples1[i]],asvtable[,samples2[i]])
asvtable[,samples1[i]
asvtable[,samples1[i]]
asvtable[,samples2[i]]
newcol = asvtable[,samples1[i]] + asvtable[,samples2[i]]
newcol
mergedasvtable <- c()#
for(i in c(1:length(samples1))){#
newcol = asvtable[,samples1[i]] + asvtable[,samples2[i]]#
cbind(mergedasvtable,samples1[i]= newcol)#
}
mergedasvtable <- c()#
for(i in c(1:length(samples1))){#
newcol = asvtable[,samples1[i]] + asvtable[,samples2[i]]#
mergedasvtable <- cbind(mergedasvtable,samples1[i]= newcol)#
}
c(1:length(samples1)
)
for(i in c(1:length(samples1))){#
newcol = asvtable[,samples1[i]] + asvtable[,samples2[i]]#
#
#mergedasvtable <- cbind(mergedasvtable, samples1[i] = newcol)#
}
newcol
newcol = t(asvtable[,samples1[i]] + asvtable[,samples2[i]])
newcol
newcol = t(t(asvtable[,samples1[i]] + asvtable[,samples2[i]]))
newcol
colnames(newcol) <- samples1[i]
head(newcol)
mergedasvtable <- c()#
for(i in c(1:length(samples1))){#
newcol = t(t(asvtable[,samples1[i]] + asvtable[,samples2[i]]))#
colnames(newcol) <- samples1[i]#
mergedasvtable <- cbind(mergedasvtable, newcol)#
}
head(mergedasvtable)
setwd("/Users/anttonalberdi/github/microbiota_adaptation_review/")#
#
#WOS needs to be exported as Excel and then exported as comma-separated csv#
#
###############
# 1- PREPARE DATA TABLES#
###############
#
#Load raw data tables#
scp_raw <- read.csv("data/scp_20211025.csv")#
wos_raw <- read.csv("data/wos_20211025.csv")#
#
#Filter columns#
scp <- scp_raw[,c("Title","Authors","Year","Document.Type","DOI","Abstract","Author.Keywords","Index.Keywords")]#
colnames(scp) <- c("Title","Authors","Year","Type","DOI","Abstract","Keywords1","Keywords2")#
wos <- wos_raw[,c("Article.Title","Authors","Publication.Year","Document.Type","DOI","Abstract","Author.Keywords","Keywords.Plus")]#
colnames(wos) <- c("Title","Authors","Year","Type","DOI","Abstract","Keywords1","Keywords2")#
#
#Unify formats#
scp[,2] <- gsub("\\.","",scp[,2])#
wos[,2] <- gsub(",","",wos[,2])#
wos[,2] <- gsub(";",",",wos[,2])#
#
#Merge datasets#
all <- rbind(scp,wos)#
#
#Merge keywords#
all$Keywords <- paste(all$Keywords1,all$Keywords2,sep="; ")#
all <- all[,c("Title","Authors","Year","Type","DOI","Abstract","Keywords")]#
all <- all[order(all$Title),]#
#
###############
# 2- FILTER DUPLICATES#
###############
#
#Filter by duplicated Title#
all.uniq <- all[!duplicated(all[,"Title"]),]#
#
#Filter by duplicated DOI#
all.uniq <- all.uniq[!duplicated(all.uniq[,"DOI"]),]#
#
#Filter by duplicated Abstract#
all.uniq <- all.uniq[!duplicated(all.uniq[,"Abstract"]),]#
#
###############
# 3- FILTER BY DOCUMENT TYPE#
###############
#
#Print all document types#
unique(all.uniq$Type)#
#
#Filter entries#
all.filt_type <- all.uniq[all.uniq$Type %in% c("Article","Letter","Article in Press","Note","Short Survey","Reprint"),]#
#
###############
# 4- FILTER BY KEYWORD#
###############
#
allkeywords <- paste(shQuote(all.filt_type$Keywords), collapse="; ")#
allkeywords <- gsub("\"","",allkeywords)#
allkeywords <- sort(unlist(strsplit(allkeywords, split="; ")))
allkeywordscounts <- table(allkeywords)
allkeywordscounts
head(allkeywordscounts)
ncol(allkeywordscounts)
head(allkeywords)
allkeywords <- paste(shQuote(all.filt_type$Keywords), collapse="; ")#
allkeywords <- gsub("\"","",allkeywords)#
allkeywords <- sort(unlist(strsplit(allkeywords, split="; ")))
head(allkeywords)
head(allkeywords,100)
head(allkeywords,1000)
allkeywords <- paste(shQuote(all.filt_type$Keywords), collapse="; ")#
allkeywords <- gsub("\"","",allkeywords)#
allkeywords <- sort(unlist(strsplit(allkeywords, split="; ")))#
allkeywordscounts <- as.data.frame(table(allkeywords))
head(allkeywordscounts)
allkeywordscounts <- allkeywordscounts[order(allkeywordscounts[,2]),]
head(allkeywordscounts)
allkeywords <- paste(shQuote(all.filt_type$Keywords), collapse="; ")#
allkeywords <- gsub("\"","",allkeywords)#
allkeywords <- sort(unlist(strsplit(allkeywords, split="; ")))#
allkeywordscounts <- as.data.frame(table(allkeywords))#
allkeywordscounts <- allkeywordscounts[order(allkeywordscounts[,2],descending=TRUE),]
help(order)
allkeywords <- paste(shQuote(all.filt_type$Keywords), collapse="; ")#
allkeywords <- gsub("\"","",allkeywords)#
allkeywords <- sort(unlist(strsplit(allkeywords, split="; ")))#
allkeywordscounts <- as.data.frame(table(allkeywords))#
allkeywordscounts <- allkeywordscounts[order(allkeywordscounts[,2],decreasing=TRUE),]
head(allkeywordscounts)
allkeywords <- paste(shQuote(all.filt_type$Keywords), collapse="; ")#
allkeywords <- gsub("\"","",allkeywords)#
allkeywords <- sort(unlist(strsplit(allkeywords, split="; ")))#
allkeywordscounts <- as.data.frame(table(allkeywords))#
allkeywordscounts <- allkeywordscounts[order(allkeywordscounts[,2],decreasing=TRUE),]#
#
#Display 100 most common keywords#
head(allkeywordscounts,100)
#Load raw data tables#
scp_raw <- read.csv("data/scp_20211025.csv")#
wos_raw <- read.csv("data/wos_20211025.csv")#
#
#Filter columns#
scp <- scp_raw[,c("Title","Authors","Year","Document.Type","DOI","Abstract","Author.Keywords","Index.Keywords")]#
colnames(scp) <- c("Title","Authors","Year","Type","DOI","Abstract","Keywords1","Keywords2")#
wos <- wos_raw[,c("Article.Title","Authors","Publication.Year","Document.Type","DOI","Abstract","Author.Keywords","Keywords.Plus")]#
colnames(wos) <- c("Title","Authors","Year","Type","DOI","Abstract","Keywords1","Keywords2")#
#
#Unify formats#
scp[,2] <- gsub("\\.","",scp[,2])#
wos[,2] <- gsub(",","",wos[,2])#
wos[,2] <- gsub(";",",",wos[,2])#
#
#Merge datasets#
all <- rbind(scp,wos)#
#
#Merge keywords#
all$Keywords <- paste(all$Keywords1,all$Keywords2,sep="; ")#
#
#Merge text case#
all$Keywords <- tolower(all$Keywords)#
#
#Rename and order#
all <- all[,c("Title","Authors","Year","Type","DOI","Abstract","Keywords")]#
all <- all[order(all$Title),]#
#
###############
# 2- FILTER DUPLICATES#
###############
#
#Filter by duplicated Title#
all.uniq <- all[!duplicated(all[,"Title"]),]#
#
#Filter by duplicated DOI#
all.uniq <- all.uniq[!duplicated(all.uniq[,"DOI"]),]#
#
#Filter by duplicated Abstract#
all.uniq <- all.uniq[!duplicated(all.uniq[,"Abstract"]),]#
#
###############
# 3- FILTER BY DOCUMENT TYPE#
###############
#
#Print all document types#
unique(all.uniq$Type)#
#
#Filter entries#
all.filt_type <- all.uniq[all.uniq$Type %in% c("Article","Letter","Article in Press","Note","Short Survey","Reprint"),]#
#
###############
# 4- FILTER BY KEYWORD#
###############
#
allkeywords <- paste(shQuote(all.filt_type$Keywords), collapse="; ")#
allkeywords <- gsub("\"","",allkeywords)#
allkeywords <- sort(unlist(strsplit(allkeywords, split="; ")))#
allkeywordscounts <- as.data.frame(table(allkeywords))#
allkeywordscounts <- allkeywordscounts[order(allkeywordscounts[,2],decreasing=TRUE),]#
#
#Display 100 most common keywords#
head(allkeywordscounts,100)
all.filt_keywords <- all.filt_type[all.filt_type$Keywords ~ keywords_to_remove,]
all.filt_keywords <- all.filt_type[all.filt_type$Keywords %in% keywords_to_remove,]
keywords_to_remove <- c("mice, inbred c57bl","mice, knockout","c57bl mouse","knockout mouse","disease models, animal","colitis","disease model",)#
#
#Filter by keywords#
all.filt_keywords <- all.filt_type[all.filt_type$Keywords %in% keywords_to_remove,]
keywords_to_remove <- c("mice, inbred c57bl","mice, knockout","c57bl mouse","knockout mouse","disease models, animal","colitis","disease model",)
keywords_to_remove <- c("mice, inbred c57bl","mice, knockout","c57bl mouse","knockout mouse","disease models, animal","colitis","disease model")
all.filt_keywords <- all.filt_type[all.filt_type$Keywords %in% keywords_to_remove,]
nrow(all.filt_keywords)
all.filt_keywords <-  filter(all.filt_type, str_detect(Keywords, paste(keywords_to_remove, collapse="|")))
library(dplyr)#
library(stringr)
all.filt_keywords <-  filter(all.filt_type, str_detect(Keywords, paste(keywords_to_remove, collapse="|")))
nrow(all.filt_keywords)
head(all.filt_keywords)
all.filt_type[all.filt_type$Title == "6-Formylindolo (3, 2-b) Carbazole (FICZ)–mediated protection of gut barrier is dependent on T cells in a mouse model of alcohol combined with burn injury",7]
all.filt_keywords <-  filter(all.filt_type, !str_detect(Keywords, paste(keywords_to_remove, collapse="|")))
nrow(all.filt_keywords)
write.csv(all.filt_type, "data/all_20211025.csv", row.names=FALSE)
all <- read.csv("data/all_20211025.csv")
head(all)
sample(1:nrow(all),100)
all.random <- all[sample(1:nrow(all),100),]
all.random
all.random <- all[sample(1:nrow(all),100,seed=15),]
help(sample)
all.random <- all[sample(1:nrow(all),200),]
all.random <- all[sample(1:nrow(all),200),]#
write.csv(all.random,"data/all_20211025_random200.csv")
write.csv(all.random,"data/all_20211025_random200.csv",row.names=FALSE)
all.random <- read.csv("data/all_20211025_random200.csv")
head(all.random)
all.random <- read.csv("data/all_20211025_random200.csv")#
all.random <- all.random[,c("Title","DOI","Keywords")]
head(all.random)
allkeywords <- paste(shQuote(all.random$Keywords), collapse="; ")#
allkeywords <- gsub("\"","",allkeywords)#
allkeywords <- sort(unlist(strsplit(allkeywords, split="; ")))
head(allkeywords)
©allkeywords <- paste(shQuote(all.random$Keywords), collapse="; ")#
allkeywords <- gsub("\"","",allkeywords)#
allkeywords <- unique(sort(unlist(strsplit(allkeywords, split="; "))))
allkeywords <- paste(shQuote(all.random$Keywords), collapse="; ")#
allkeywords <- gsub("\"","",allkeywords)#
allkeywords <- unique(sort(unlist(strsplit(allkeywords, split="; "))))
head(allkeywords)
matrix(ncol=length(allkeywords+3),nrow=nrow(all.random))
matrix(ncol=length(allkeywords)+3,nrow=nrow(all.random))
colnames(all.matrix) <- c(c("Title","DOI","Result"),allkeywords)
all.matrix <- matrix(ncol=length(allkeywords)+3,nrow=nrow(all.random))#
colnames(all.matrix) <- c(c("Title","DOI","Result"),allkeywords)
head(all.matrix)
rownames(all.matrix) <- all.random$DOI
head(all.matrix)
